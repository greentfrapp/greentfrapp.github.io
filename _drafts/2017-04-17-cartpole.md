---
layout: post
title:  "Code for tackling Cartpole"
date:   2017-04-16 01:10:00 +0800
categories: 
comments: true
mathjax: true
---

**TL;DR** In response to the <a href ="">Cartpole</a> exercise in OpenAI's <a href="">Requests for Research</a>, I coded some solutions for solving the exercise, including <a href="#random_search">random search</a>, hill climbing and policy gradient. The solutions use minimal Python libraries - `gym` for loading the environment and `numpy` for matrix operations.

[//]: # (Add caption picture here)
[//]: # (Maybe add a TOC with #links and use div as bookmarks)

---

### Cartpole

The <a href="">Cartpole</a> environment is a simple one. Visually, you have a pole standing on a moving cart and you try to balance the pole by moving the cart left or right. Sort of like balancing an umbrella on your palm but in a 1D environment. Every game/espisode lasts for as long as the pole can be balanced - the episode ends if the pole veers too far off the vertical axis or the episode exceeds 200 steps.

[//]: # (Image of Cartpole environment)

All `gym` environments allow you to run an action with `env.step(action)` which returns a tuple of `(state,reward,done,info)`:

- `state` is a vector of floats that represent the new state of the environment after taking the action - one of the floats may give the x-position of the cart, another one may give the angle of the pole with respect to the cart etc. **The `gym` documentation denotes this as `observation` but I prefer to use the term `state` since that means half the letters to type; I will use `state` from hereon.**

- `reward` gives the reward as defined by the game - in this case, every step without terminating gives a reward of 1.0. So if we manage to balance the pole for 10 steps we will have a reward of 1.0 for 10 steps. We have to sum this ourselves to get the total reward ie. 10*1.0 = 10.0.

- `done` is a bool signifying True if the game/episode is over.

- `info` is a dict containing some debugging information.

The general shape of the code will look something like this:
```python
# reset the env and receive the first state
state = env.reset()
total_reward = 0
while True:
	# use state to generate an action using my algorithm
	action = my_algorithm(state)
	# run a new action and receive returned tuple
	state,reward,done,info = env.step(action)
	total_reward += reward
	# end the loop if game is over ie. done == True
	if done: break
```

### Reinforcement Learning

In reinforcement learning (RL) we define a *policy*, which tells us what action to take given a state. For example, if I really hate vegetables, then my policy for eating may be to ignore my meal if it contains vegetables and to start eating if it does not contain vegetables. In the Cartpole environment, the policy should return one of the actions (0 for left and 1 for right) given the state vector.

[//]: # (Image describing looping)

Furthermore, we use the reward received as feedback to change the policy. Returning to my diet, I may receive negative reward in the form of poor bowel movements and hence amend my policy to accept more vegetables. In Cartpole, the policy is updated periodically with the aim of maximizing the total reward per game.

### Random Search

[//]: # (Maybe too mathematical; try to simplify this)

Possibly the simplest RL algorithm, random search just tries random policies and keeps the one that results in the highest total reward. 

We earlier defined a policy as something that tells us what action to take given a state. 
In this case, the action set is {0,1}: 0 for left and 1 for right.
The state is a vector of four floats $$\begin{bmatrix}s_1\\s_2\\s_3\\s_4\end{bmatrix}$$.
Then we can reframe the policy as a function that maps the state vector to the action set.

$$\pi(S) = A \forall S \in \Bbb{R}^4, A \in \{0,1\}$$

One way to define policy $$\pi()$$ is to set it as a simple linear combination of the state elements ie.

$$\pi(S,W) = w_1s_1 + w_2s_2 + w_3s_3 + w_4s_4$$

Alternatively in matrix form:

$$\pi(S,W) = W^\top S$$

Assuming that there exists a $$W$$ such that $$\pi(S,W)$$ succeeds, we then just have to find the optimal $$W$$.

In random search, we basically randomly set $$W$$ and test the policy in the environment, keeping the best $$W$$.











