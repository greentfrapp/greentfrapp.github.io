---
layout: post
title:  "Dissecting LSTM Networks"
date:   2018-05-20 18:04:57 +0800
categories:
comments: true
---

This is meant to be a follow-up to Christopher Olah's great blog post [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). If you haven't, go read that first!

The accompanying code for this post can be found [here](https://github.com/greentfrapp/lstm-dissect)!

### Once I caught a fish alive

*\*Beep\** "One!" 
*\*Beep\** "Two!"
*\*Boop\** "Two!"
*\*Bzzt\** "Zero!"
*\*Beep\** "One!"

Counting is a simple enough task for most humans and yet deceptively challenging to learn for neural networks. In this article, we use a simple counting task to dissect the inner workings of an LSTM network.

We define the **Counting Task** as follows:

Consider a sequence of elements where each element can be -1, 1 or 0. Running alongside the sequence is a counter that counts the number of 1s that have appeared and restarts from 0 whenever a -1 appears. The counter provides the final label for each sequence. 

For instance, the sample sequence 1, 1, 1, 1, 1, 1, 1, 1, -1, 0 will have a label of 0.

### Learning to Forget

Recurrent neural networks (RNN) mainly consist of a persistent hidden state. In the context of the Counting Task, for the ideally-trained RNN, this implies the following changes when each element/step is entered.

- **1** - Changes the hidden state - Output increases by 1
- **0** - Leaves the hidden state unchanged - Output unchanged
- **-1** - Resets the hidden state - Output is 0

### Implementing an RNN without RNN

A(n? Just 'A' from now on) RNN is actually a simple feedforward network in disguise. 

For an input with *M* dimensions and a hidden state with *N* dimensions, the RNN is just a simple dense network that takes in *M+N*-dimensional input and generates *N*-dimensional output.

Consider the trivial example of 1D input and hidden state.



### Implementing an LSTM without LSTM

While Chris Olah's blog post (see above) provides an amazingly clear explanation of the LSTM, there remain several details and challenges in actually implementing the LSTM network without the convenience of a `LSTM`/`LSTMCell` class.

![Schematic for LSTM from Chris Olah's post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)

The basic structure for an LSTM can be implemented as follows, using Tensorflow's `dense` class:

{% highlight python %}
with tf.variable_scope("lstm"):
	# forget gate
	self.forget_gate = tf.layers.dense(
		inputs=tf.concat([self.hidden_state, self.input], axis=1),
		units=self.units,
		activation=tf.sigmoid,
		kernel_initializer=tf.truncated_normal_initializer(.0,.01),
		name="forget_gate",
	)

	# input gate
	self.input_gate_filter = tf.layers.dense(
		inputs=tf.concat([self.hidden_state, self.input], axis=1),
		units=self.units,
		activation=tf.sigmoid,
		kernel_initializer=tf.truncated_normal_initializer(.0,.01),
		name="input_gate_filter",
	)
	self.input_gate_update = tf.layers.dense(
		inputs=tf.concat([self.hidden_state, self.input], axis=1),
		units=self.units,
		activation=tf.nn.relu,
		kernel_initializer=tf.truncated_normal_initializer(.0,.01),
		name="input_gate_update",
	)

	# output gate
	self.output_gate = tf.layers.dense(
		inputs=tf.concat([self.hidden_state, self.input], axis=1),
		units=self.units,
		activation=tf.sigmoid,
		kernel_initializer=tf.truncated_normal_initializer(.0,.01),
		name="output_gate",
	)

	# new cell state
	self.new_cell_state = self.forget_gate * self.cell_state + self.input_gate_filter * self.input_gate_update

	# new hidden state
	self.new_hidden_state = self.output_gate * tf.nn.relu(self.new_cell_state)

{% endhighlight %}


### Draft Notes
Maybe retheme this post to link RNNs and CNNs, placing less of an emphasis on dissecting the RNN/LSTM and more on a unified discussion of neural nets

### A Unified Theory of Nets

*dense nets, recurrent nets, convolutional nets, fishing nets, hairnets, bayonets*

[Convolutional, Long Short-term Memory, Fully Connected Deep Neural Networks](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43455.pdf)

[An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/pdf/1803.01271.pdf)

The convolutional net is a generalized form of dense and recurrent nets. Consider a time series dataset, we can use an RNN to process each timestep. Alternatively, we can use a CNN where the stride goes along the time axis. 

On the other hand, with a regular sample that has N features, given a 1D kernel of length N (ie. no stride) with multiple filters, this corresponds to a dense network layer, where each filter corresponds to a node.
