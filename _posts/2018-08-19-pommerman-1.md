---
layout: post
title:  "Pommerman: Part I"
subtitle: "Exploring Pommerman"
date:   2018-08-19 00:00:00
tags: ai rl code
comments: True
---

<img alt='Pommerman GIF' src='https://www.pommerman.com/static/media/pommerman.abbcd943.gif'/>

*Playing Pommerman (GIF taken from the Pommerman [website](https://www.pommerman.com/))*

I recently came across [Pommerman](https://www.pommerman.com/) as one of eight competitions in the [NIPS 2018 Competition Track](https://nips.cc/Conferences/2018/CompetitionTrack).

I had previously done some work on [Starcraft2](https://github.com/greentfrapp/pysc2-RLagents) and thought it would be a good idea to brush up my RL knowledge and also learn more about multi-agent RL with Pommerman. Also, the top two agents in Pommerman will win a [Titan V GPU](https://www.pommerman.com/competitions).

Read the rules of Pommerman [here](https://www.pommerman.com/about)!

---

## Installing Pommerman

Installation is straightforward, just follow the instructions in the [documentation](https://github.com/MultiAgentLearning/playground/tree/master/docs) or below, to install the `pommerman` library via `pip`:

{% highlight bash %}
$ git clone https://github.com/MultiAgentLearning/playground
$ cd playground
$ pip install -U .
{% endhighlight %}

**Note the period `.`!** Missed it out when I was trying to install it and wasted a few minutes of my life.

On a side note, this also means that if the organizers update the library, we just have to pull the latest repository and then run `pip install -U .` again.

After the installation, run a quick test by going into the `examples` folder and running the [`simple_ffa_run.py`](https://github.com/MultiAgentLearning/playground/blob/master/examples/simple_ffa_run.py) script.

{% highlight bash %}
$ cd examples
$ python3 simple_ffa_run.py
{% endhighlight %}

The game should appear and run with four bots, just like the GIF at the beginning of this post.

---

## Agents

Here we take a look at the default [agents](https://github.com/MultiAgentLearning/playground/tree/master/pommerman/agents) that the library comes with.

#### [BaseAgent](https://github.com/MultiAgentLearning/playground/blob/master/pommerman/agents/base_agent.py)

First, note that all agents (ie. any agents that we build) should inherit from this class, since it defines certain important methods that work with the game environment.

{% highlight python %}
def __init__(self, character=characters.Bomber):
	self._character = character

def __getattr__(self, attr):
	return getattr(self._character, attr)
{% endhighlight %}

The `__init__` and `__getattr__` methods indicate that the `BaseAgent` class acts like a wrapper around the [`characters.Bomberman`](https://github.com/MultiAgentLearning/playground/blob/master/pommerman/characters.py) class.

This means that after `init_agent` is called in `BaseAgent` (by the environment), we will be able to access attributes of `Bomberman`, such as `self.agent_id`, `self.ammo`, `self.can_kick` etc. These will be helpful for determining the state of the agent. (These attributes are not provided by the observation dict returned from the environment. See Environment section below for more details.)

{% highlight python %}
def act(self, obs, action_space):
	raise NotImplementedError()
{% endhighlight %}

*Note to self: TIL about `NotImplementedError`*

Your agent (that inherits `BaseAgent`) has to implement this method!

This method is named `act` but a better name is probably `get_action`, because we do not actually perform the action here. Instead, the environment calls this method for every agent (total of 4), to get their actions, before performing them all at once.

As such, this method takes as arguments `obs` and `action_space` and should return an `action`.

**`obs`**

This is a Python `dict` containing the state of the environment. The Pommerman [description](https://www.pommerman.com/about) seems to be outdated, but I explain `obs` in detail [here](https://github.com/greentfrapp/pommerman-agents/blob/master/notes/observations.txt).

**`action_space`**

This is actually not used much, since there seems to be no illegal actions (moving into a wall is the same as not moving). For the curious, this is actually a [`gym.spaces.Discrete`](https://github.com/openai/gym/blob/master/gym/spaces/discrete.py) object. The most important thing to know is that there are 6 possible actions and there are no illegal actions at any time.

**`action`**

This just has to be an `int` in the range of 0 to 5:

- `0` Stop
- `1` Up
- `2` Down
- `3` Left
- `4` Right
- `5` Bomb

Note: `5` plants the bomb at the agent's position. If you are rendering the environment, the bomb will not be shown when it is planted, until the agent moves away. The Can Kick ability will NOT kick the bomb while the agent is on top of the bomb.

#### [RandomAgent](https://github.com/MultiAgentLearning/playground/blob/master/pommerman/agents/random_agent.py) 

This is a good example of a bare-minimum agent. We just have to inherit from `BaseAgent` and implement the `act` method to return an `int` from the action space (0 to 5).

#### [SimpleAgent](https://github.com/MultiAgentLearning/playground/blob/master/pommerman/agents/simple_agent.py)

> This is a baseline agent. After you can beat it, submit your agent to compete.

As suggested, this is a hand-engineered agent provided by the organizers as a baseline.

While it's called `SimpleAgent`, I think quite a bit of thought was put into designing the strategy of this agent. That being said, it is not great and seems to suicide quite a bit.

As mentioned in the [documentation](https://github.com/MultiAgentLearning/playground/tree/master/docs):

> The SimpleAgent is very useful as a barometer for your own efforts. Four SimpleAgents playing against each other have a win rate of ~18% each with the remaining ~28% of the time being a tie. Keep in mind that it *can* destroy itself. That can skew your own results if not properly understood.

This also means that having an agent do nothing but return `0` (stop) will have a positive probability of winning against three other `SimpleAgents`. I am not going to go into detail but it is definitely worth taking a look at the script to see what sort of strategy the agent uses.

#### [TensorForceAgent](https://github.com/MultiAgentLearning/playground/blob/master/pommerman/agents/tensorforce_agent.py)

This agent uses the [TensorForce](https://github.com/reinforceio/tensorforce) library. I haven't actually tried it out yet and the script mentioned this is a work-in-progress. It is also supposed to be used with the [`train_with_tensorforce.py`](https://github.com/MultiAgentLearning/playground/blob/master/pommerman/cli/train_with_tensorforce.py) script.

Note: the `act` method here returns `None`. This is because the action is predicted 'externally' in the [`train_with_tensorforce.py`](https://github.com/MultiAgentLearning/playground/blob/master/pommerman/cli/train_with_tensorforce.py) script. I will use a similar trick for implementing my agents.

#### [PlayerAgent](https://github.com/MultiAgentLearning/playground/blob/master/pommerman/agents/player_agent.py)

This agent can be called for ***actual real-live*** humans to play Pommerman. Wow!

This can be done by replacing `agent_list` in the [`simple_ffa_run.py`](https://github.com/MultiAgentLearning/playground/blob/master/examples/simple_ffa_run.py) script with the following:

{% highlight python %}
agent_list = [
    agents.SimpleAgent(),
    agents.PlayerAgent(agent_control="arrows"), # arrows to move, space to lay bomb
    agents.SimpleAgent(),
    agents.PlayerAgent(agent_control="wasd"), # W,A,S,D to move, E to lay bomb
]
{% endhighlight %}

There are two modes for two humans to duke it out. Although, I've tried it and I must say, it's not going to win Game of the Year anytime.

This might be an interesting way for evaluating a trained agent. Also, there is a facinating [documentation](https://github.com/MultiAgentLearning/playground/blob/master/pommerman/agents/player_agent.py) at the beginning of the script that discusses issues with human control.

---

That's all for now. I will update with another post soon with an implementation of a custom agent.
