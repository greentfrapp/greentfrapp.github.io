---
layout: post
title:  "Dissecting LSTM Networks"
date:   2018-05-20 18:04:57 +0800
categories:
comments: true
---

This is meant to be a follow-up to Christopher Olah's great blog post [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). If you haven't, go read that first!

The accompanying code for this post can be found [here](https://github.com/greentfrapp/lstm-dissect)!

### Once I caught a fish alive

*\*Beep\** "One!" 
*\*Beep\** "Two!"
*\*Boop\** "Two!"
*\*Bzzt\** "Zero!"
*\*Beep\** "One!"

Counting is a simple enough task for most humans and yet deceptively challenging to learn for neural networks. In this article, we use a simple counting task to dissect the inner workings of an LSTM network.

We define the **Counting Task** as follows:

Consider a sequence of elements where each element can be -1, 1 or 0. Running alongside the sequence is a counter that counts the number of 1s that have appeared and restarts from 0 whenever a -1 appears. The counter provides the final label for each sequence. 

For instance, the sample sequence 1, 1, 1, 1, 1, 1, 1, 1, -1, 0 will have a label of 0.

### Learning to Forget

Long Short Term Memory (LSTM) networks comprise of a persistent cell state and a hidden state. In the context of the Counting Task, for the ideally-trained LSTM, this implies the following changes when each element/step is entered.

- **1** - Changes the cell state - Output increases by 1
- **0** - Leaves the cell state unchanged - Output unchanged
- **-1** - Resets the cell state - Output is 0

### Implementing an LSTM without LSTM

While Chris Olah's blog post (see above) provides an amazingly clear explanation of the LSTM, there remain several details and challenges in actually implementing the LSTM network without the convenience of a `LSTM`/`LSTMCell` class.

![Schematic for LSTM from Chris Olah's post]({{ "http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" }})

The basic structure for an LSTM can be implemented as follows, using Tensorflow's `dense` class:

{% highlight python %}
with tf.variable_scope("lstm"):
	# forget gate
	self.forget_gate = tf.layers.dense(
		inputs=tf.concat([self.hidden_state, self.input], axis=1),
		units=self.units,
		activation=tf.sigmoid,
		kernel_initializer=tf.truncated_normal_initializer(.0,.01),
		name="forget_gate",
	)

	# input gate
	self.input_gate_filter = tf.layers.dense(
		inputs=tf.concat([self.hidden_state, self.input], axis=1),
		units=self.units,
		activation=tf.sigmoid,
		kernel_initializer=tf.truncated_normal_initializer(.0,.01),
		name="input_gate_filter",
	)
	self.input_gate_update = tf.layers.dense(
		inputs=tf.concat([self.hidden_state, self.input], axis=1),
		units=self.units,
		activation=tf.nn.relu,
		kernel_initializer=tf.truncated_normal_initializer(.0,.01),
		name="input_gate_update",
	)

	# output gate
	self.output_gate = tf.layers.dense(
		inputs=tf.concat([self.hidden_state, self.input], axis=1),
		units=self.units,
		activation=tf.sigmoid,
		kernel_initializer=tf.truncated_normal_initializer(.0,.01),
		name="output_gate",
	)

	# new cell state
	self.new_cell_state = self.forget_gate * self.cell_state + self.input_gate_filter * self.input_gate_update

	# new hidden state
	self.new_hidden_state = self.output_gate * tf.nn.relu(self.new_cell_state)

{% endhighlight %}




