---
layout: post
title:  "Dissecting LSTM Networks"
date:   2018-05-20 18:04:57 +0800
categories:
comments: true
---

This is meant to be a follow-up to Christopher Olah's great blog post [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). If you haven't, go read that first!

The accompanying code for this post can be found [here](https://github.com/greentfrapp/lstm-dissect)!

### Once I caught a fish alive

*\*Beep\** "One!"

*\*Beep\** "Two!"

*\*Boop\** "Two!"

*\*Bzzt\** "Zero!"

*\*Beep\** "One!"

Counting is a simple enough task for most humans and yet deceptively challenging to learn for neural networks. In this article, we use a simple counting task to dissect the inner workings of an LSTM network.

We define the Counting Task as follows:

Consider a sequence of elements where each element can be -1, 1 or 0. Running alongside the sequence is a counter that counts the number of 1s that have appeared and restarts from 0 whenever a -1 appears. The counter provides the final label for each sequence. For instance, the sample sequence 1, 1, 1, 1, 1, 1, 1, 1, -1, 0 will have a label of 0.

### Learning to Forget

Long Short Term Memory (LSTM) networks comprise of a persistent cell state and a hidden state. In the context of the Counting Task, this implies the following changes when each element/step is input.

- 1 - Changes the cell state - Output increases by 1
- 0 - Leaves the cell state unchanged - Output unchanged
- -1 - Resets the cell state - Output is 0

