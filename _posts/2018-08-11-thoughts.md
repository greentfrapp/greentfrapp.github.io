---
layout: post
title:  "AI and Humanities"
subtitle: "Thoughts as an AI researcher venturing into the humanities."
date:   2017-08-09
tags: social ai commentary
comments: True
---

<div class='note note-left'>
	This post marks my transition from a full-time AI researcher to a full-time urban science student. When I wrote my response to the 'What attracted you to this program?' question in the application form for the course, I was surprised to find that I had many thoughts about the mixing of the 'hard' and 'soft' sciences. I figure it will also be fun to look back on this post and see what has changed after finishing my urban science course.
</div>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">This sounds like a loaded question but... Why don&#39;t more people who talk about AI $subject (eg, ethics, policy, strategy) actually engage with foundational technical literature? What is the plausible justification for not habitually skimming Arxiv?</p>&mdash; Jack Clark (@jackclarkSF) <a href="https://twitter.com/jackclarkSF/status/997512143695241217?ref_src=twsrc%5Etfw">May 18, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

---

I used to find the term **social science** oxymoronic. Not that I felt either humanities or sciences were inferior to the other, but that I thought the two were mutually exclusive. I enjoyed my share of visual arts, literature and cinematography, as well as a healthy dose of math and science (staples of the Singaporean education). 

Recurrent words, symbols and themes in literature, the mise-en-sc√®ne, cuts and pacing in cinematography - these invoke raw feelings that do not necessarily have to be dissected to be impactful. On the other hand, math and science classes taught calculus, geometry, reactions and patterns and trends, dictated by gradients and formulas and rules (although I must say that I fully agree with Paul Lockhart's [A Mathematician's Lament](https://www.mimuw.edu.pl/~pawelst/rzut_oka/Zajecia_dla_MISH_2011-12/Lektury_files/LockhartsLament.pdf)!). 

One of the books that changed my mind about this was Poor Economics by Abhijit Banerjee and Esther Duflo. 

<div class='note note-left'>
	Here's a <a href='https://www.youtube.com/watch?v=0zvrGiPkVcs'>link</a> to the TED talk by Esther Duflo on Poor Economics.
</div>

A primary theme of the book was the use of Randomized Controlled Trials (RCTs) for understanding the effects of aid measures on the poor. RCTs have been traditionally employed for clinical and scientific trials. But Banerjee and Duflo demonstrated that RCTs could be used to shed light on the 'soft' psychological factors influencing the success of aid measures.

It was fascinating to see how traditional scientific techniques and sociological investigations could be complementary and lead to new innovations.

---

<div class='note note-left'>
	Meredith Whittaker is a research scientist at New York University and Co-Director of the [AI Now Institute](https://ainowinstitute.org/), which is dedicated to understanding social implications of AI. 
</div>

<div class='note note-right'>
	Jack Clark is OpenAI's Strategy & Communications Director. He maintains the awesome Import AI newsletter. Check it out [here](https://jack-clark.net/)!
</div>

Just as I was applying to the MSc in Urban Science, Policy and Planning at SUTD, I came across Jack Clark's tweet above, as well as Meredith Whittaker's response (below). The tweets resonated strongly with me, an AI research assistant applying to a humanities graduate program.

<blockquote class="twitter-tweet" data-lang="en" style='margin:auto;'><p lang="en" dir="ltr">Fair point that can just as easily, and more urgently, be inverted: why don&#39;t those building AI for $subject actually engage with the foundational $subject literature? Including the histories of ethics, discrimination, etc in $subject domain... <a href="https://t.co/SPkSBCxWzz">https://t.co/SPkSBCxWzz</a></p>&mdash; Meredith Whittaker (@mer__edith) <a href="https://twitter.com/mer__edith/status/998211595879833602?ref_src=twsrc%5Etfw">May 20, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

I think there are two main parts to both tweets. For conciseness and due to personal experience, I will refer to Meredith's tweet. However, I believe that the underlying issues are mirrored in Jack's question. Of course, the following is based on my personal experience and your mileage may vary.

#### Why don't AI practitioners engage with the domain that they are designing AI systems for?

I worked as an AI researcher at a cybersecurity lab. Prior to that, I worked also worked as an AI engineer at a chatbot company. In both cases, the extent of my knowledge in the subject domains (cybersecurity and conversation) was superficial and limited to what was strictly necessary.

Many commentary pieces warn against using AI as a hammer and treating every problem as a nail (just Google 'AI is not a hammer'). However, I find that AI engineers tend to exhibit a more subtle version of this. The AI engineer begins with a bag of AI tools, some tried-and-tested and others bleeding-edge. Faced with the problem, the AI engineer immediately throws out a variety of tools. When asked which one is likely to perform well, the engineer might hazard a guess. But the truth is, we are not that certain either. 

> It's all rather empirical I'm afraid.

Missing from all this is the initial stage to understand the problem from first principles. There is a general sentiment that AI is a magic tool that *just works*, which is frequently reinforced by news articles reporting new breakthroughs in hyperbolic fashion. Unfortunately, I find that such sentiments are shared by many AI practitioners themselves.

Related to the religious faith in AI, companies looking for AI solutions underestimate the amount of information and analysis required to design a great system. Problem settings are often variations of "Okay I've got a bunch of data. Can you feed it into the neural network?" Often there is little time allocated for interdisciplinary investigation and a virtual wall between AI practitioners and domain experts.

Another aspect of this is AI researchers often prefer working on more general innovations rather than problem-specific implementations. The motivation behind this being that general innovations are more publishable at AI conferences and journals, while problem-specific implementations are more likely to end up in domain-specific journals.

Finally, it is simple human nature to be lazy. Furthermore, when companies are demanding superficial AI solutions (just for the looks of it), general algorithms do work *good enough*. In such cases, the AI engineer has little incentive to really understand the problem at hand.

#### Should AI practitioners engage with the domain that they are designing AI systems for?

This question is somewhat rhetorical, since the answer (duh, yes) is implied in both Meredith and Jack's tweets. But I think it might be worth thinking about on a deeper level.

We do not expect electrical engineers to be well-informed about foundational architectural studies. Neither do we expect architects to be experts at designing full circuit diagrams for buildings. A well-designed building relies on both parties (and many more) doing their job and, just as importantly, communicating with each other clearly.

---

My personal response to Meredith's tweet takes the form of my enrolment in the MSc in Urban Science, Policy and Planning. I had originally ventured into AI hoping to one day be able to apply AI research to helping solve social problems. Yet, I admittedly did not attempt to understand the nature of such problems.

On the bright side, with my (albeit limited) experience as an AI researcher, I am now far better equipped to think about how AI can be used. Keeping that in mind, I am excited to learn and understand the social problems that afflict the world and design solutions for fixing them.




